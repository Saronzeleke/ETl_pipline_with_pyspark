# ETl_pipline_with_pyspark
Big data analytics and business inteligence
Technology Stack:

Orchestration: Prefect (v2.14.5) with task retries, flow visualization

Distributed Processing: Apache PySpark (v3.5.0) for scalable transformations

Analytical Database: DuckDB (v0.9.2) for high-performance querying

BI & Visualization: Microsoft Power BI Desktop

Data Quality: Loguru for structured logging, PySpark data validation

ğŸ“ Repository Structure
ETL_PIPLINE_WITH_PYSPARK/
â”œâ”€â”€ workflow/
â”‚   â””â”€â”€ pipeline.py                    # ğŸ›ï¸ Prefect orchestration DAG
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract.py                     # ğŸ“¥ Data download & Spark loading
â”‚   â”œâ”€â”€ transform.py                   # ğŸ”„ Cleaning, enrichment, aggregation
â”‚   â””â”€â”€ load.py                        # ğŸ’¾ DuckDB loading & Parquet export
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.py                    # âš™ï¸ Centralized configuration
â”‚   â””â”€â”€ spark_config.py                # âš¡ Spark session management
â”œâ”€â”€ data/                              # ğŸ—ƒï¸ Data directory (auto-generated)
â”‚   â”œâ”€â”€ raw/                           # Source files (Parquet, CSV, JSON)
â”‚   â”œâ”€â”€ processed/                     # Transformed datasets
â”‚   â””â”€â”€ exports/                       # Final exports for BI consumption
â”œâ”€â”€ requirements.txt                   # ğŸ“¦ Python dependencies
â”œâ”€â”€ README.md                         # This documentation
â””â”€â”€ dashboard_screenshots/            # ğŸ“¸ Power BI dashboard visuals